{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from collections import defaultdict\n",
    "from args import get_parser\n",
    "from data_utils import MAX_ROBERTA_LENGTH\n",
    "from style_dataset import (InverseParaphraseDatasetText,\n",
    "                           ParaphraseDatasetText)\n",
    "from transformers import (WEIGHTS_NAME, AdamW, GPT2Config, GPT2LMHeadModel,\n",
    "                          GPT2Tokenizer, get_linear_schedule_with_warmup)\n",
    "from utils import GPT2ParentModule, init_gpt2_model\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"additional_special_tokens\": [\"<dense-vectors>\", \"<tokens>\", \"<verb>\", \"<ARG0>\", \"<ARG1>\", \"<global-dense-vectors>\"],\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从路径中加载文本数据和label文件\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
    "    if not args.prefix_input_type.startswith(\"original\"):\n",
    "        dataset = InverseParaphraseDatasetText(\n",
    "            tokenizer=tokenizer,\n",
    "            args=args,\n",
    "            evaluate=evaluate,\n",
    "            split=\"dev\" if evaluate else \"train\"\n",
    "        )\n",
    "    else:\n",
    "        dataset = ParaphraseDatasetText(\n",
    "            tokenizer=tokenizer,\n",
    "            args=args,\n",
    "            evaluate=evaluate,\n",
    "            split=\"dev\" if evaluate else \"train\"\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, '{}-*'.format(checkpoint_prefix)))\n",
    "    if len(glob_checkpoints) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    ordering_and_checkpoint_path = []\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)\n",
    "\n",
    "\n",
    "def save_model(gpt2_model, output_dir, args, tokenizer=None):\n",
    "    # Take care of distributed/parallel training\n",
    "    if not issubclass(type(gpt2_model), GPT2LMHeadModel):\n",
    "        model_to_save = gpt2_model.gpt2\n",
    "    else:\n",
    "        model_to_save = gpt2_model\n",
    "    model_to_save = model_to_save.module if hasattr(\n",
    "        model_to_save, 'module') else model_to_save\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "    if tokenizer:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "## 训练的关键代码\n",
    "def train(args, gpt2_model, train_dataset, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        try:\n",
    "            tb_writer = SummaryWriter(logdir=\"runs/summary_%s\" % args.job_id)\n",
    "        except:\n",
    "            tb_writer = SummaryWriter(log_dir=\"runs/summary_%s\" % args.job_id)\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Update the model definition in case RoBERTa is training\n",
    "    model = gpt2_model.gpt2\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    # extra layer_norm.weight for com\n",
    "    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "\n",
    "    grouped_parameters = [\n",
    "        {\n",
    "            'params': [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay': args.weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay': 0.0\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(grouped_parameters,\n",
    "                      lr=float(args.learning_rate),\n",
    "                      eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    # this is necessary to ensure multi-GPU training happens since the gpt2_model.gpt2 pointer has been set to the model without the DDP wrapper\n",
    "    gpt2_model.gpt2 = model\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    global_step = 0\n",
    "    loss_metrics = {\n",
    "        \"lm\": {\"current\": 0.0, \"previous\": 0.0}\n",
    "    }\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            #loss的第一个key的value就是每一个预测的token的交叉熵。\n",
    "            loss = gpt2_model(batch)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                for k, v in loss.items():\n",
    "                    loss[k] = v.mean()\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                for k, v in loss.items():\n",
    "                    loss[k] = v / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss[\"lm\"], optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss[\"lm\"].backward()\n",
    "\n",
    "            # Update the metrics for Tensorboard logging\n",
    "            for metric_type, metric_vals in loss_metrics.items():\n",
    "                metric_vals[\"current\"] += loss[metric_type].item()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                # Update the generator or the discriminator optimizer\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:\n",
    "                        results = evaluate(args, gpt2_model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "\n",
    "                    tb_writer.add_scalar('learning_rate', scheduler.get_lr()[0], global_step)\n",
    "\n",
    "                    for metric_type, metric_vals in loss_metrics.items():\n",
    "                        tb_writer.add_scalar(\n",
    "                            '%s_loss' % metric_type,\n",
    "                            (metric_vals[\"current\"] - metric_vals[\"previous\"]) / args.logging_steps,\n",
    "                            global_step\n",
    "                        )\n",
    "                        metric_vals[\"previous\"] = metric_vals[\"current\"]\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = 'checkpoint'\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, '{}-{}'.format(checkpoint_prefix, global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "\n",
    "                    save_model(gpt2_model, output_dir, args, global_step, tokenizer=tokenizer)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "    return global_step, loss_metrics[\"lm\"][\"current\"] / global_step\n",
    "\n",
    "def evaluate(args, gpt2_model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        gpt2_model = torch.nn.DataParallel(gpt2_model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    total_instances = 0\n",
    "\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        curr_loss = gpt2_model.evaluate(batch)\n",
    "        eval_loss += curr_loss\n",
    "        total_instances += batch[\"suffix_style\"].shape[0]\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置\n",
    "原代码中使用argparser传参数。这里需要在notebook执行，参考[from args import get_parser]()将需要用的参数调整为默认值。\n",
    "同时locakrank调整为-1（单机单卡执行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "dataset_path = \"../datasets/new_dataset\"\n",
    "\n",
    "\n",
    "class CONFIG:\n",
    "    output_dir = f\"{dataset_path}/saved_models/model_shakespeare_0\"\n",
    "    model_type = \"gpt2\"\n",
    "    model_name_or_path = '../paraphraser_gpt2_large'  #gpt2-large模型路径\n",
    "    data_dir = dataset_path\n",
    "    save_steps = 5000\n",
    "    logging_steps = 20\n",
    "    save_total_limit = -1\n",
    "    job_id = \"test\"\n",
    "    num_train_epochs = 3\n",
    "    gradient_accumulation_steps = 2\n",
    "    per_gpu_train_batch_size = 5\n",
    "    limit_examples = None\n",
    "    learning_rate = 5e-5\n",
    "    tokenizer_name = \"\"\n",
    "    global_dense_feature_list = \"none\"\n",
    "    specific_style_train = \"0,1,2,3\"  #4种不同的语气风格\n",
    "    optimizer = \"adam\"\n",
    "    weight_decay = 0\n",
    "    adam_epsilon = 1e-8\n",
    "    target_style_override = \"none\"\n",
    "    config_name = \"\"\n",
    "    cache_dir = None  #第一次下载模型的保存路径\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_delete_old = True\n",
    "    evaluate_during_training = False\n",
    "    per_gpu_train_batch_size = 3\n",
    "    gradient_accumulation_steps = 1\n",
    "    warmup_steps = 0\n",
    "    local_rank = -1\n",
    "    no_cuda = False\n",
    "    extra_embedding_dim = 768\n",
    "    do_lower_case = False\n",
    "    prefix_input_type = \"paraphrase_250\"  #调整成经过释义的保存文件(custom dataset第4步产生的文件）的前缀\n",
    "    max_steps = -1\n",
    "    fp16 = False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    max_grad_norm = 1.0\n",
    "    output_dir = 'output_dir'\n",
    "\n",
    "\n",
    "args= CONFIG()\n",
    "args.data_dir\n",
    "if (os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir):\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "\n",
    "args.device = device\n",
    "args.device\n",
    "args.seed =100\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型-GPT2-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name_or_path = '/data/dengwc/github/style-transfer-paraphrase/paraphraser_gpt2_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                        cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "# Adding an extra embedding dimension for style/content vectors\n",
    "config.extra_embedding_dim = args.extra_embedding_dim\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "                                            do_lower_case=args.do_lower_case,\n",
    "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "\n",
    "model = model_class.from_pretrained(args.model_name_or_path,\n",
    "                                    from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "                                    config=config,\n",
    "                                    cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的GPT2ParentModule是作者定义的类，目的是更好地调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.to(args.device)\n",
    "gpt2_model = GPT2ParentModule(args=args, gpt2=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集\n",
    "\n",
    "```\n",
    "每个样本（文本和他的label）用了下面的格式\n",
    "{\n",
    "    \"sentence\": torch.tensor(sentence), #将释义的token和风格的token拼接为一个sentence 调用并训练gpt2这种原本根据前文预测下一个token的模型需要这么做。\n",
    "    \"instance_number\": item,   \n",
    "    \"label\": torch.tensor(label),  \n",
    "      #label中对应释义token的部分为-100，让模型忽视这个label，对应风格token的是与风格token的完全相同。\n",
    "    \"segment\": torch.tensor(segment), #区分释义token和风格token50258是释义句子 50259是风格句子\n",
    "    \"suffix_style\": suffix_style,     # 目标风格 \n",
    "    \"original_style\": original_style,  # 在逆释义训练中中就是释义风格（无风格）\n",
    "    \"init_context_size\": init_context_size,\n",
    "    \"global_dense_vectors\": global_dense_vectors,\n",
    "    \"metadata\": self.examples[item].dict[\"metadata\"],\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22138/22138 [00:07<00:00, 2860.42it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "# Training\n",
    "if args.do_train:\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    \n",
    "    # 加载数据集 \n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': tensor([50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263,\n",
       "         50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263, 50263,\n",
       "         50263, 50263, 50263, 50263,  1169, 15260,  1641,  9141,   262,  5079,\n",
       "         40051,  5136,   338,  3452, 17127,    11,   366,  1544,  4005,   306,\n",
       "           347,  5042,    25, 30958,   290,   262,  7835, 39440,  1883,   526,\n",
       "         50264, 13787, 15260,   290,   465,  3656,   402,   786,   293, 13319,\n",
       "          6607,  9141,   262,  5079, 27639,   329,   262, 21609,  9594,   286,\n",
       "          3683,   338, 40051,  5136,   338,  3452, 15866,    11,   366,  1544,\n",
       "          4005,   306,   347,  5042,    25, 30958,   290,   262,  7835, 39440,\n",
       "          1883,   526,   383,  7880,   329,   262,   968,  4492, 13104,   290,\n",
       "           262, 50265]),\n",
       " 'instance_number': 22127,\n",
       " 'label': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 13787, 15260,   290,   465,  3656,   402,   786,   293, 13319,\n",
       "          6607,  9141,   262,  5079, 27639,   329,   262, 21609,  9594,   286,\n",
       "          3683,   338, 40051,  5136,   338,  3452, 15866,    11,   366,  1544,\n",
       "          4005,   306,   347,  5042,    25, 30958,   290,   262,  7835, 39440,\n",
       "          1883,   526,   383,  7880,   329,   262,   968,  4492, 13104,   290,\n",
       "           262, 50265]),\n",
       " 'segment': tensor([50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259]),\n",
       " 'suffix_style': 0,\n",
       " 'original_style': 0,\n",
       " 'init_context_size': 51,\n",
       " 'global_dense_vectors': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32),\n",
       " 'metadata': 'suffix_style = 0, original_style = 0'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[22127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step, tr_loss = train(args, gpt2_model, train_dataset, tokenizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     parser = get_parser(\"finetuning\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if (os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir):\n",
    "#         raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "\n",
    "#     # Setup CUDA, GPU & distributed training\n",
    "#     if args.local_rank == -1 or args.no_cuda:\n",
    "#         device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "#         args.n_gpu = torch.cuda.device_count()\n",
    "#     else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "#         torch.cuda.set_device(args.local_rank)\n",
    "#         device = torch.device(\"cuda\", args.local_rank)\n",
    "#         torch.distributed.init_process_group(backend='nccl')\n",
    "#         args.n_gpu = 1\n",
    "#     args.device = device\n",
    "\n",
    "#     # Setup logging\n",
    "#     logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                         datefmt='%m/%d/%Y %H:%M:%S',\n",
    "#                         level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "#     logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "#                    args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "#     # Set seed\n",
    "#     set_seed(args)\n",
    "\n",
    "#     # Load pretrained model and tokenizer\n",
    "#     if args.local_rank not in [-1, 0]:\n",
    "#         # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "#         torch.distributed.barrier()\n",
    "\n",
    "#     config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "#     config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "#                                           cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "#     # Adding an extra embedding dimension for style/content vectors\n",
    "#     config.extra_embedding_dim = args.extra_embedding_dim\n",
    "#     tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "#                                                 do_lower_case=args.do_lower_case,\n",
    "#                                                 cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "\n",
    "#     model = model_class.from_pretrained(args.model_name_or_path,\n",
    "#                                         from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "#                                         config=config,\n",
    "#                                         cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "#     tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     model.to(args.device)\n",
    "\n",
    "#     gpt2_model = GPT2ParentModule(args=args, gpt2=model)\n",
    "\n",
    "#     if args.local_rank == 0:\n",
    "#         torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "#     logger.info(\"Training/evaluation parameters %s\", args)\n",
    "#     # Training\n",
    "#     if args.do_train:\n",
    "#         if args.local_rank not in [-1, 0]:\n",
    "#             torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "#         train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
    "\n",
    "#         if args.local_rank == 0:\n",
    "#             torch.distributed.barrier()\n",
    "\n",
    "#         global_step, tr_loss = train(args, gpt2_model, train_dataset, tokenizer)\n",
    "#         logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "#     if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "\n",
    "#         output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "#         if not os.path.exists(output_dir) and args.local_rank in [-1, 0]:\n",
    "#             os.makedirs(output_dir)\n",
    "#         save_model(gpt2_model, output_dir, args, global_step, tokenizer)\n",
    "\n",
    "#         gpt2_model, tokenizer = init_gpt2_model(checkpoint_dir=args.output_dir,\n",
    "#                                                 args=args,\n",
    "#                                                 model_class=model_class,\n",
    "#                                                 tokenizer_class=tokenizer_class)\n",
    "\n",
    "#     # Evaluation\n",
    "#     if args.do_eval and args.local_rank in [-1, 0]:\n",
    "#         eval_done = False\n",
    "#         all_results = {}\n",
    "#         top_checkpoint = None\n",
    "#         patience = 0\n",
    "\n",
    "#         while not eval_done:\n",
    "#             checkpoints = []\n",
    "#             if not args.evaluate_specific:\n",
    "#                 checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/checkpoint-*/' + WEIGHTS_NAME, recursive=True)))\n",
    "#                 logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "#                 # Sort checkpoints according to the step number\n",
    "#                 if len(checkpoints) > 0:\n",
    "#                     checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "#             else:\n",
    "#                 checkpoints.append(args.evaluate_specific)\n",
    "\n",
    "#             checkpoints = [x for x in checkpoints if x not in all_results]\n",
    "\n",
    "#             # Count the number of while loop iterations no new checkpoints were found\n",
    "#             if len(checkpoints) == 0:\n",
    "#                 patience += 1\n",
    "#             else:\n",
    "#                 patience = 0\n",
    "\n",
    "#             logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "#             for checkpoint in checkpoints:\n",
    "#                 prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "\n",
    "#                 gpt2_model, _ = init_gpt2_model(checkpoint_dir=checkpoint,\n",
    "#                                                 args=args,\n",
    "#                                                 model_class=model_class)\n",
    "\n",
    "#                 result = evaluate(args, gpt2_model, tokenizer, prefix=prefix)\n",
    "#                 all_results[checkpoint] = result[\"perplexity\"]\n",
    "\n",
    "#             sorted_results = [(k, v) for k, v in all_results.items()]\n",
    "#             sorted_results.sort(key=lambda x: x[1].item())\n",
    "\n",
    "#             if not args.evaluate_specific and args.do_delete_old and len(sorted_results) > args.save_total_limit:\n",
    "#                 logger.info(\"Deleting worse checkpoints...\")\n",
    "#                 # delete all but the top save_total_limit checkpoints\n",
    "#                 for res in sorted_results[args.save_total_limit:]:\n",
    "#                     if os.path.exists(res[0]):\n",
    "#                         logger.info(\"Deleting {}...\".format(res[0]))\n",
    "#                         shutil.rmtree(res[0])\n",
    "\n",
    "#             # move top checkpoint to root directory\n",
    "#             if not args.evaluate_specific and len(sorted_results) > 0 and sorted_results[0][0] != top_checkpoint:\n",
    "#                 command = \"cp {}/* {}\".format(sorted_results[0][0], args.output_dir)\n",
    "#                 logger.info(\"executing {}...\".format(command))\n",
    "#                 subprocess.check_output(command, shell=True)\n",
    "#                 top_checkpoint = sorted_results[0][0]\n",
    "\n",
    "#             sorted_results_summary = \"\\n\".join([\"{} = {:.4f}\".format(x[0], x[1]) for x in sorted_results])\n",
    "#             logger.info(\"Top checkpoints:\\n{}\".format(sorted_results_summary))\n",
    "\n",
    "#             if args.eval_frequency_min == 0 or args.evaluate_specific or patience > args.eval_patience:\n",
    "#                 eval_done = True\n",
    "#             else:\n",
    "#                 logger.info(\"Sleeping for {:d} minutes...zzzz...\".format(args.eval_frequency_min))\n",
    "#                 time.sleep(args.eval_frequency_min * 60)\n",
    "\n",
    "#     return all_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,output_dir=args.output_dir,args=args,tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
